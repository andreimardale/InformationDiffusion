{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import copy\n",
    "import dill\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## in script mode, read filenames from arguments\n",
    "import sys\n",
    "\n",
    "# print('Number of arguments:', len(sys.argv), 'arguments.')\n",
    "# print('Argument List:', str(sys.argv))\n",
    "# INPUT_FILE = str(sys.argv[1])\n",
    "# OUTPUT_FILE = str(sys.argv[2])\n",
    "\n",
    "# comment the next two lines for Notebook mode\n",
    "INPUT_FILE = \"F4_T12_data.csv\"\n",
    "OUTPUT_FILE = \"F4_improved.db\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data in from the csv...\n",
      "Done reading data from csv\n",
      "{0.0: 7, 1.0: 2}\n",
      "{0.0: 6, 1.0: 3}\n",
      "{0.0: 7, 1.0: 2}\n",
      "{0.0: 7, 1.0: 2}\n",
      "{0.0: 8, 1.0: 1}\n",
      "{0.0: 7, 1.0: 2}\n",
      "{0.0: 9}\n",
      "{0.0: 6, 1.0: 3}\n",
      "{0.0: 7, 1.0: 2}\n",
      "{0.0: 7, 1.0: 2}\n",
      "7.1\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "dill.load_session(\"F4_report.db\")\n",
    "\n",
    "print(\"Reading data in from the csv...\")\n",
    "personalities = pd.read_csv(\"data/F4_T4_data.csv\")\n",
    "uniq_data = []\n",
    "uniq_handle = {}\n",
    "for index, row in personalities.iterrows():\n",
    "    if row[0] in uniq_handle:\n",
    "        continue\n",
    "    else:\n",
    "        uniq_handle[row[0]] = 1\n",
    "        uniq_data.append(row)\n",
    "\n",
    "print(\"Done reading data from csv\")\n",
    "        \n",
    "data = pd.DataFrame(uniq_data)\n",
    "data = data.drop(data.columns[[0]], axis=1)  # df.columns is zero-based pd.Index\n",
    "\n",
    "all_features = data.iloc[:,0:(len(data.columns) - 1)].values.tolist()\n",
    "all_labels = data.iloc[:,(len(data.columns) - 1)].values.tolist()\n",
    "\n",
    "# print(all_features)\n",
    "\n",
    "against = 0\n",
    "brexit = 0\n",
    "neutral = 0\n",
    "\n",
    "for i in range(0, 10):\n",
    "    predicted_labels = trained_clfs[i][\"xgboost\"].predict(np.array(all_features))\n",
    "    unique, counts = np.unique(predicted_labels, return_counts=True)\n",
    "    ddd = (dict(zip(unique, counts)))\n",
    "    against += ddd[0]\n",
    "    brexit += ddd[1]\n",
    "    neutral += ddd[2]\n",
    "    print(ddd)\n",
    "\n",
    "print(against / 10)\n",
    "print(brexit / 10)\n",
    "print(neutral / 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data in from the csv...\n",
      "Done reading data from csv\n",
      "Running with 10 folds\n",
      "Splitting data into training and testing sets\n",
      "1577 176\n",
      "1577 176\n",
      "1577 176\n",
      "1578 175\n",
      "1578 175\n",
      "1578 175\n",
      "1578 175\n",
      "1578 175\n",
      "1578 175\n",
      "1578 175\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading data in from the csv...\")\n",
    "personalities = pd.read_csv(INPUT_FILE)\n",
    "uniq_data = []\n",
    "uniq_handle = {}\n",
    "for index, row in personalities.iterrows():\n",
    "    if row[0] in uniq_handle:\n",
    "        continue\n",
    "    else:\n",
    "        uniq_handle[row[0]] = 1\n",
    "        uniq_data.append(row)\n",
    "\n",
    "print(\"Done reading data from csv\")\n",
    "        \n",
    "data = pd.DataFrame(uniq_data)\n",
    "data = data.drop(data.columns[[0]], axis=1)  # df.columns is zero-based pd.Index \n",
    "\n",
    "\n",
    "NUM_FOLDS=10\n",
    "print(\"Running with\", NUM_FOLDS,\"folds\")\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=20)\n",
    "TRAIN_DATA=0\n",
    "TRAIN_LABELS=1\n",
    "TEST_DATA=2\n",
    "TEST_LABELS=3\n",
    "\n",
    "TRUE_LABEL = 0\n",
    "PREDICTED_LABEL = 1\n",
    "\n",
    "all_features = data.iloc[:,0:(len(data.columns) - 1)].values.tolist()\n",
    "all_labels = data.iloc[:,(len(data.columns) - 1)].values.tolist()\n",
    "\n",
    "index = 0\n",
    "\n",
    "print(\"Splitting data into training and testing sets\")\n",
    "datasets = [[[],[],[],[]] for _ in range(NUM_FOLDS)]\n",
    "for train, test in skf.split(all_features, all_labels):\n",
    "    print(len(train), len(test))\n",
    "    datasets[index][TRAIN_DATA] = [all_features[i] for i in train]\n",
    "    datasets[index][TRAIN_LABELS] = [all_labels[i] for i in train]\n",
    "    datasets[index][TEST_DATA] = [all_features[i] for i in test]\n",
    "    datasets[index][TEST_LABELS] = [all_labels[i] for i in test]\n",
    "\n",
    "    \n",
    "    for li1 in datasets[index][TEST_DATA]:\n",
    "        for li2 in datasets[index][TRAIN_DATA]:\n",
    "            if li1 == li2:\n",
    "                print(\"ERROR: (DUPLICATE FOUND)\", li1)\n",
    "                  \n",
    "    index += 1\n",
    "    \n",
    "no_features = len(pd.DataFrame(all_features).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_parameters = {}\n",
    "# Only for randomised hyper parameter tuning\n",
    "tuned_parameters['KNN'] = {'n_neighbors' : list(range(2,200,5)), \n",
    "                            'leaf_size' : list(range(2, 200, 5)),\n",
    "                            'p' : [1,2]\n",
    "                           } \n",
    "tuned_parameters['random_forest'] = {'n_estimators' : list(range(5, 200, 5)),\n",
    "                                     'min_samples_leaf' : list(range(5, 200, 3)), \n",
    "                                     'criterion' : ['gini', 'entropy'], \n",
    "                                     'min_samples_split' : list(range(2, 200, 5)),\n",
    "                                     'class_weight':[\"balanced\"]\n",
    "                                    }\n",
    "# tuned_parameters['gradient_boosting'] = {'learning_rate' : [0.0001, 0.001, 0.1], \n",
    "#                                          'n_estimators' : list(range(20, 200, 2)), \n",
    "#                                          'max_features' : list(range(2, no_features))\n",
    "#                                         }\n",
    "# tuned_parameters['xgboost'] = {'n_estimators' : list(range(2, 300, 2)), \n",
    "#                                'max_depth' : list(range(2, 100, 2)), \n",
    "#                                'learning_rate' : [0.0001, 0.001, 0.01, 0.1],\n",
    "#                                'subsample':[0.6,0.7,0.75,0.8,0.85,0.9,0.95,1.0]\n",
    "#                               }\n",
    "\n",
    "clfs = defaultdict(list)\n",
    "clfs = {}\n",
    "confusion_matrices = defaultdict(list)\n",
    "clfs['random_forest'] = RandomizedSearchCV(RandomForestClassifier(), \n",
    "                                           tuned_parameters['random_forest'], \n",
    "                                           n_jobs = -1, \n",
    "                                           n_iter=200,\n",
    "                                           cv = 5,\n",
    "                                           verbose = 1)\n",
    "# clfs['gradient_boosting'] = RandomizedSearchCV(GradientBoostingClassifier(), \n",
    "#                                               tuned_parameters['gradient_boosting'], \n",
    "#                                               n_jobs = -1,\n",
    "#                                               n_iter=500,\n",
    "#                                               cv = 5,\n",
    "#                                               verbose = 1)\n",
    "# clfs['xgboost'] = RandomizedSearchCV(estimator = XGBClassifier(), \n",
    "#                                      param_distributions = tuned_parameters['xgboost'], \n",
    "#                                      n_jobs = -1,\n",
    "#                                      n_iter=500,\n",
    "#                                      cv = 5,\n",
    "#                                      verbose = 1)\n",
    "# clfs['logistic_regression'] = LogisticRegressionCV(Cs = [0.0001, 0.001, 0.1, 1.0, 10.0, 100, 1000],\n",
    "#                                                    fit_intercept=True, \n",
    "#                                                    n_jobs = -1,\n",
    "#                                                    max_iter = 500,\n",
    "#                                                    cv = 5,\n",
    "#                                                    verbose = 1)\n",
    "#clfs['KNN'] = RandomizedSearchCV(KNeighborsClassifier(), \n",
    "#                                  tuned_parameters['KNN'], \n",
    "#                                  n_jobs = -1, \n",
    "#                                  n_iter=500,\n",
    "#                                  cv = 5,\n",
    "#                                  verbose = 1)\n",
    "\n",
    "true_labels = {}\n",
    "predicted_labels = {}\n",
    "scores = {}\n",
    "for classifier in clfs:\n",
    "    scores[classifier] = [[] for _ in range(NUM_FOLDS)]\n",
    "    true_labels[classifier] = [[] for _ in range(NUM_FOLDS)]\n",
    "    predicted_labels[classifier] = [[] for _ in range(NUM_FOLDS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting for all models: \n",
      "--> Training random_forest , fold:  0\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 104 tasks      | elapsed:    5.5s\n",
      "[Parallel(n_jobs=-1)]: Done 460 tasks      | elapsed:   20.0s\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed:   40.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   42.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  1\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 144 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done 744 tasks      | elapsed:   26.4s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   37.2s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  2\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 132 tasks      | elapsed:    5.6s\n",
      "[Parallel(n_jobs=-1)]: Done 732 tasks      | elapsed:   28.9s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   41.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  3\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 703 tasks      | elapsed:   29.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   40.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  4\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 209 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 662 tasks      | elapsed:   27.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   41.5s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  5\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 197 tasks      | elapsed:    7.8s\n",
      "[Parallel(n_jobs=-1)]: Done 647 tasks      | elapsed:   25.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   40.1s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  6\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 263 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=-1)]: Done 740 tasks      | elapsed:   29.5s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   39.4s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  7\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 255 tasks      | elapsed:    9.7s\n",
      "[Parallel(n_jobs=-1)]: Done 705 tasks      | elapsed:   27.9s\n",
      "[Parallel(n_jobs=-1)]: Done 993 out of 1000 | elapsed:   39.9s remaining:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   39.9s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  8\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  76 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 376 tasks      | elapsed:   16.4s\n",
      "[Parallel(n_jobs=-1)]: Done 876 tasks      | elapsed:   39.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   44.6s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> Training random_forest , fold:  9\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 314 tasks      | elapsed:   14.2s\n",
      "[Parallel(n_jobs=-1)]: Done 814 tasks      | elapsed:   36.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1000 out of 1000 | elapsed:   44.4s finished\n"
     ]
    }
   ],
   "source": [
    "print(\"Training and predicting for all models: \")\n",
    "trained_clfs = [[] for _ in range(NUM_FOLDS)]\n",
    "for i, dataset in enumerate(datasets):\n",
    "    training_data = dataset[TRAIN_DATA]\n",
    "    training_labels = dataset[TRAIN_LABELS]\n",
    "    testing_data = dataset[TEST_DATA]\n",
    "    testing_labels = dataset[TEST_LABELS]\n",
    "    \n",
    "    ## initialise classifiers from list\n",
    "    trained_clfs[i] = copy.deepcopy(clfs)\n",
    "\n",
    "    for classifier in trained_clfs[i]:\n",
    "        print(\"--> Training\", classifier,\", fold: \",i)\n",
    "        trained_clfs[i][classifier].fit(np.array(training_data), training_labels)\n",
    "        predicted_labels[classifier][i] = trained_clfs[i][classifier].predict(np.array(testing_data))\n",
    "        true_labels[classifier][i] = testing_labels\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(testing_labels, predicted_labels[classifier][i], average='macro')\n",
    "        scores[classifier][i] = [prec, rec, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_forest [0.24881643 0.39814815 0.27078832]\n"
     ]
    }
   ],
   "source": [
    "all_scores = {}\n",
    "names = []\n",
    "for classifier in scores:\n",
    "    score = np.array([0,0,0])\n",
    "    for i in range(len(scores[classifier])):\n",
    "        score = score + np.array(scores[classifier][i])/NUM_FOLDS\n",
    "    all_scores[classifier] = score  \n",
    "\n",
    "for classifier in all_scores:\n",
    "    print(classifier, all_scores[classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully completed everything!\n"
     ]
    }
   ],
   "source": [
    "dill.dump_session(OUTPUT_FILE)\n",
    "print(\"Successfully completed everything!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
